{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "pMhYyhhAkhN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wh4GZtr8av0m"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset and Tokenizer"
      ],
      "metadata": {
        "id": "y1gLQDbVksRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Abirate/english_quotes\")\n",
        "dataset_split = dataset['train'].train_test_split(test_size=0.1, seed=35)\n",
        "\n",
        "train_data = dataset_split['train']\n",
        "val_dataset = dataset_split['test']\n",
        "\n",
        "model_name = 'gpt2'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402,
          "referenced_widgets": [
            "df66cf8b608249e8bb75bb3a8790db91",
            "e987c3f16c1a4fbb95e239c167528594",
            "c4ec05a1b8f44ad3a88eb4459c6b13af",
            "98929e57d37a4b1e8c3d4371b11fd629",
            "8e988a6079394743b8a9ed6400e03db6",
            "60d593e0fb634acfae97179ec308d1fd",
            "4a851bad455a4ac8a343f60045be19d4",
            "af5d258fdbcf493787e264f6bdb12579",
            "55fee2c09ab145e0a8f7cb77e600677a",
            "06774c3c7c5d4ca8af65f4169e3a0a4a",
            "54b9fa2771534372bc77f94a3b516ef0",
            "453689f5e42b4a0c8c44946cdef264b7",
            "7fd31cfd8c674416805ad9f8c9d4c9ad",
            "d33476a74fbb44be81aba566f9ef2044",
            "5e54840738b44c358a2e9eef3d8119b5",
            "6a63de3ea1cc4263afc6d5cdbd2602e1",
            "8f7e7229d2d14f989b4ee663847fd1c6",
            "8487207c7bd447cfba1aedaabe057cf7",
            "81a831834bd84d44ba702c4f85b140b0",
            "9d3098fcacbd4b598d7670d6eb34a359",
            "00452c6badc44ea8b3bf79f886d6e345",
            "728eea3b3ff445afb8724e0ef7f2c36a",
            "578fdbd2a7b64f81b558b5a85c8716b2",
            "d15c5d74ee074a9a8dbfd4e5d75e654d",
            "71d51b2e21db40fcb5b3696b36918b0d",
            "e2a725fd48624d468631fe1c6ece8529",
            "2ae3a759fae84f74bcb313fb01c08082",
            "faad74d6f12f4852b779a3c2547f222c",
            "e8ba0cd79f98411faebc851cba1d14db",
            "eb31d05c80bd405d9272536648eae26d",
            "48bb9fc529bc4eadad6820746588e6b3",
            "5f9a85954adb45ce9eec2f3db5596525",
            "cc4ac3c802fa490f8a9c034f25f9e978",
            "549f74e5bad4447691124dc233a16f23",
            "bb61be01028c4a80aa005655d80e981d",
            "c3e7af3df24a47d084da5e2ea31b0439",
            "5a5059b065264f4f8766a31853105e37",
            "f713485b2e904674a1490038681a972b",
            "e429012f6aa84eaa9434b0078406dea6",
            "ebcdab1a213b49c787ab857b579f3bcd",
            "25252379d7034d75bf31cd7453e31549",
            "37ae382e3023490b9584bf3b5848c20e",
            "448b80a687954cef85b7044c0d5cedd4",
            "dd1458b39e2143c38e2b79a7ab07358a",
            "7a1838affafb4360901b14ed722af2d8",
            "5b3740fcc75e4dc8ab613ece75343645",
            "4f318c70053a43788cc658c5a6ed1bfc",
            "0751b91419e049709c6d86a73da538fa",
            "7d96d17d66f144a6bddac082d709385f",
            "362e76759afa485c98b79904b5bf2b38",
            "1bf76ebbb24d4523afd8df0f9ec31a7d",
            "7fd752c11da4442cae79164afa1ef970",
            "37b81ceb41fd4c2b810b2efd3887333e",
            "a562fbd1155744e7a8510153ef5d7c84",
            "bb3d353aeb444288928ca0cda105e49f",
            "2ab522e563f641b4adbe0309af32c51f",
            "ce6e6d36d7af4e348344f73bf2db5bc0",
            "c495bb5ccea84982881169f1d49212ad",
            "d3701a08843e46b1b21c8f83b6c67947",
            "b2ffdc1659274b3683fed15b7b1809fe",
            "b0358b7f62864c529438009bd9203837",
            "6de2835d1f094127aeab1cf034a41299",
            "df21095a38614d3b86da2e9f725659a7",
            "b1cc01031f2e4f4f808e2e0b21c664da",
            "19d7fe2375f24b37927267f45bed5997",
            "76324a64885a4aeb9e37c80fd12bb0ab",
            "5ecc507657e647f782a1a189daec5303",
            "6d61d8ee93d24d0995448febe265f9df",
            "d57b771ce6474d2cb98c7d143ef4adc1",
            "24952dcc735449ffa9767abef8f5ddc8",
            "e3b1d31bd15543f29cd7111b714cc79a",
            "a1b6addbce5d430d954bd16adeefbf62",
            "0f2276aff54f47baacfb870e07b42080",
            "61c5096d9d5741a8be263534ee5e8fb1",
            "79483440717f41b6884f25797794a266",
            "113fee2717804759b4ec7e371cc8e810",
            "05fae9dc538d42b99c42ad6c442982d9",
            "555c4616497441bc84d54caac2568e6b",
            "236275ff8d8641a4a7c391e4e5a2bae3",
            "a61e8ebbc05447099c586ab8a01fb88b",
            "beb4e74b1a78410e900b57d7e7888921",
            "1eb3d13b27ae4ff09f0214671bb1b8e9",
            "b5446e90701e46a0a349aefe559a674f",
            "834a37e4638f4a539c8d3d22e754cf27",
            "226894e07fad45778bcacb0ab6bf2f78",
            "aa59ed9b38a04320a59ec70050c6b8ba",
            "829370604e3d481a995d939950909227",
            "cbc23896149a4de3abc32681a9610584"
          ]
        },
        "id": "W7Nd5DGla3b_",
        "outputId": "0abbdc2c-6f4f-4bda-a0a0-bdd27a342190"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df66cf8b608249e8bb75bb3a8790db91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "quotes.jsonl:   0%|          | 0.00/647k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "453689f5e42b4a0c8c44946cdef264b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2508 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "578fdbd2a7b64f81b558b5a85c8716b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "549f74e5bad4447691124dc233a16f23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a1838affafb4360901b14ed722af2d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ab522e563f641b4adbe0309af32c51f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ecc507657e647f782a1a189daec5303"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "555c4616497441bc84d54caac2568e6b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize the Data"
      ],
      "metadata": {
        "id": "oFNQv3_Yk0v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenized(batch):\n",
        "  tokenized = tokenizer(batch['quote'], padding='max_length', max_length=64, truncation=True)\n",
        "\n",
        "  tokenized['labels'] = tokenized['input_ids'].copy()\n",
        "  return tokenized\n",
        "\n",
        "train_data = train_data.map(tokenized, batched=True)\n",
        "val_dataset = val_dataset.map(tokenized, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "9c1eadee8e4647909f8e5b85f3dff16a",
            "6c5fac0538b8449a90cf06fbbd6c83bd",
            "410dcb9599ad4e68aab53123c49294b7",
            "30f84bf5ed1247188769be18ec63f6be",
            "0295545407f64e29b63346621c469475",
            "758a407e34214a188f57351e28c43da2",
            "14484befba194bce92f2991767da2026",
            "131d8be6ee6141d2b50a394a866cb131",
            "2dacc0a31c5544fd8697cc4c59c61669",
            "93da5bc7f43a421db0b6aa47781310fb",
            "191c1f4faa0a4d4b84c4f9493831854b",
            "d609b9c6b5c345e5b6dbaf6ec62cf999",
            "3f18d177bcef4eeeab43f1551909f2ac",
            "a7884839d2da445aae24404c98117512",
            "2f286054d49e4b81842f13fc3449eb08",
            "645dc9eeecb0480bbe24caf7979323e1",
            "006b1a5e020247a098ae3cadc17711d9",
            "51a5231acd0a4f99ae35bef09f619093",
            "cbd69cf022d149b58dfe064adbd7c3fa",
            "8ac3042d5dff4f1d96a20f4f3232b72a",
            "e82a034c24154faf9d70af68c22a2957",
            "80e4a2f81f374cd38bfc24cd1b31348a"
          ]
        },
        "id": "IRD5s0Wma3Yu",
        "outputId": "9013ad5f-517a-4cd0-df8f-a21c34a4f2b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2257 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c1eadee8e4647909f8e5b85f3dff16a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/251 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d609b9c6b5c345e5b6dbaf6ec62cf999"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "PqITsPTClIJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype = torch.float16,\n",
        "    device_map = \"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "18a083fec5b443f396d36a3aaba33281",
            "f38654b9dc6c4d6facbe6c46bd6d7190",
            "7ebdc04ecd4844678a31fff7e276d8ca",
            "d93b5b6d94334338a93d7bb3dea087c9",
            "68db5e4b02504eff84d86ac580129f40",
            "29c1f811cd50471d9c75221b0c15b8cc",
            "6ebf28b7a7444f6ab062113ad3080b04",
            "e7aa72c182944f8b8fd47fd2c157e498",
            "b2a3248af21e4b3c929314aa6b27bfe7",
            "ab95d07294ca4e74876d776fa86a336d",
            "d60efecddb9e4694bdd94d81a2e84579",
            "945ab14cbd4643bcbf101276637fb584",
            "fd2d3d7e489f4732863458e8872ce835",
            "3d0f81425de64cf08ea795c892e1fdc4",
            "00e3349f3c6b4cd0a6f766c2366d9d2a",
            "2773f896ac8243c982af4b79b11db86a",
            "a354a0db146541d7ad5260edb7378e88",
            "1219344eb8944d6ab72d6c34ba6082c0",
            "46ddb47ec637406c923f115a41ae32bf",
            "29c733b098ea4cdab89c121ee941a741",
            "4237938136ab4265a8fa20d53d995af5",
            "3fd063bc4843465a9a53401383993288"
          ]
        },
        "id": "Axqspo8ba3V6",
        "outputId": "43cf2899-2d68-4889-f426-c4d86cb88362"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18a083fec5b443f396d36a3aaba33281"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "945ab14cbd4643bcbf101276637fb584"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lora config"
      ],
      "metadata": {
        "id": "fD4nMBDbk8As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=['c_attn'],\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM'\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMTUNiQwa3Tc",
        "outputId": "997aca8e-9966-446b-a2e6-460016a96c2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2156: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Arguments"
      ],
      "metadata": {
        "id": "dC84hHsRlQab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-lm\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    logging_steps=10,\n",
        "    save_steps=10,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=5,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "1H3TwOzMa3QM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer + Training the model"
      ],
      "metadata": {
        "id": "KQA-gT05lbBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EN0K3g5va3M0",
        "outputId": "b233aaae-f1ea-4cdb-dbc6-bd54a1f0cca9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3757520364.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1415' max='1415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1415/1415 08:20, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>6.251800</td>\n",
              "      <td>6.500947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.429800</td>\n",
              "      <td>5.178981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>4.283200</td>\n",
              "      <td>3.589407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.010400</td>\n",
              "      <td>2.162248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.163100</td>\n",
              "      <td>2.004877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.094800</td>\n",
              "      <td>1.932420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.148800</td>\n",
              "      <td>1.860501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.968900</td>\n",
              "      <td>1.805996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.976300</td>\n",
              "      <td>1.755809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.818100</td>\n",
              "      <td>1.713689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.911600</td>\n",
              "      <td>1.677821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.857700</td>\n",
              "      <td>1.649695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.730200</td>\n",
              "      <td>1.631489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.677200</td>\n",
              "      <td>1.619361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.546100</td>\n",
              "      <td>1.612422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.720500</td>\n",
              "      <td>1.608774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.631500</td>\n",
              "      <td>1.605349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.627400</td>\n",
              "      <td>1.604545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.902300</td>\n",
              "      <td>1.602547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.833700</td>\n",
              "      <td>1.597144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.647300</td>\n",
              "      <td>1.593360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.803100</td>\n",
              "      <td>1.593454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.509000</td>\n",
              "      <td>1.590017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.711400</td>\n",
              "      <td>1.586121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.572100</td>\n",
              "      <td>1.588159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.749800</td>\n",
              "      <td>1.583369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.665400</td>\n",
              "      <td>1.583007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.831700</td>\n",
              "      <td>1.587794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.587000</td>\n",
              "      <td>1.584662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.669600</td>\n",
              "      <td>1.581305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.425300</td>\n",
              "      <td>1.580229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.657600</td>\n",
              "      <td>1.580229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.603700</td>\n",
              "      <td>1.578892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.472900</td>\n",
              "      <td>1.577933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.461000</td>\n",
              "      <td>1.580778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.580500</td>\n",
              "      <td>1.577208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.671500</td>\n",
              "      <td>1.573047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.779400</td>\n",
              "      <td>1.572429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.717100</td>\n",
              "      <td>1.571114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.532900</td>\n",
              "      <td>1.572733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.801600</td>\n",
              "      <td>1.568691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.632900</td>\n",
              "      <td>1.569358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.623800</td>\n",
              "      <td>1.568719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.659700</td>\n",
              "      <td>1.568399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.643800</td>\n",
              "      <td>1.566079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.558700</td>\n",
              "      <td>1.565649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.791500</td>\n",
              "      <td>1.564573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.628900</td>\n",
              "      <td>1.563313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.730300</td>\n",
              "      <td>1.562373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.752600</td>\n",
              "      <td>1.561517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>1.670400</td>\n",
              "      <td>1.562782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.787200</td>\n",
              "      <td>1.560688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>1.687600</td>\n",
              "      <td>1.559821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.598300</td>\n",
              "      <td>1.560078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.412300</td>\n",
              "      <td>1.560395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>1.746000</td>\n",
              "      <td>1.558257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>1.727600</td>\n",
              "      <td>1.558021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.701200</td>\n",
              "      <td>1.557701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>1.464100</td>\n",
              "      <td>1.557698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.404800</td>\n",
              "      <td>1.558836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>1.676700</td>\n",
              "      <td>1.556722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>1.584900</td>\n",
              "      <td>1.557676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>1.631900</td>\n",
              "      <td>1.558004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>1.569500</td>\n",
              "      <td>1.558682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.471800</td>\n",
              "      <td>1.558497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.706700</td>\n",
              "      <td>1.558339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>1.613900</td>\n",
              "      <td>1.557829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>1.807100</td>\n",
              "      <td>1.555879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>1.623500</td>\n",
              "      <td>1.555748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.323200</td>\n",
              "      <td>1.556827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>1.730300</td>\n",
              "      <td>1.556606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>1.845500</td>\n",
              "      <td>1.556378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>1.643200</td>\n",
              "      <td>1.554817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>1.654400</td>\n",
              "      <td>1.553533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.595400</td>\n",
              "      <td>1.554889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>1.767200</td>\n",
              "      <td>1.555022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>1.731300</td>\n",
              "      <td>1.555195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>1.484700</td>\n",
              "      <td>1.556748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>1.738200</td>\n",
              "      <td>1.556895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.565400</td>\n",
              "      <td>1.556215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>1.515800</td>\n",
              "      <td>1.556063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>1.739200</td>\n",
              "      <td>1.555302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>1.444300</td>\n",
              "      <td>1.554674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>1.844200</td>\n",
              "      <td>1.553406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.715300</td>\n",
              "      <td>1.553787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>1.674500</td>\n",
              "      <td>1.554919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>1.780700</td>\n",
              "      <td>1.554354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>1.699900</td>\n",
              "      <td>1.553188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>1.639400</td>\n",
              "      <td>1.553686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.504300</td>\n",
              "      <td>1.554594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>1.687700</td>\n",
              "      <td>1.556403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>1.506800</td>\n",
              "      <td>1.556427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>1.684900</td>\n",
              "      <td>1.555062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>1.823700</td>\n",
              "      <td>1.554993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.653000</td>\n",
              "      <td>1.554888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>1.541300</td>\n",
              "      <td>1.554928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>1.574100</td>\n",
              "      <td>1.554322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>1.482600</td>\n",
              "      <td>1.554411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>1.461300</td>\n",
              "      <td>1.555460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.695700</td>\n",
              "      <td>1.555381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>1.668000</td>\n",
              "      <td>1.554940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>1.441800</td>\n",
              "      <td>1.555589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>1.742000</td>\n",
              "      <td>1.554710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>1.671900</td>\n",
              "      <td>1.553457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.513200</td>\n",
              "      <td>1.553211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>1.690400</td>\n",
              "      <td>1.553290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>1.462000</td>\n",
              "      <td>1.554403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>1.478000</td>\n",
              "      <td>1.554618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>1.725900</td>\n",
              "      <td>1.554322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.566000</td>\n",
              "      <td>1.553492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>1.719000</td>\n",
              "      <td>1.552771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>1.611100</td>\n",
              "      <td>1.551896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>1.486900</td>\n",
              "      <td>1.551545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>1.607300</td>\n",
              "      <td>1.551791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.768200</td>\n",
              "      <td>1.551287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>1.879100</td>\n",
              "      <td>1.550598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>1.676900</td>\n",
              "      <td>1.550846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>1.690400</td>\n",
              "      <td>1.550660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>1.441200</td>\n",
              "      <td>1.551234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.416200</td>\n",
              "      <td>1.551899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>1.591200</td>\n",
              "      <td>1.552269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>1.571600</td>\n",
              "      <td>1.551961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>1.730100</td>\n",
              "      <td>1.551419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>1.619800</td>\n",
              "      <td>1.551039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.783700</td>\n",
              "      <td>1.551167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>1.561500</td>\n",
              "      <td>1.551022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>1.618700</td>\n",
              "      <td>1.551314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>1.501600</td>\n",
              "      <td>1.551455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>1.706200</td>\n",
              "      <td>1.551442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.714400</td>\n",
              "      <td>1.551290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>1.735500</td>\n",
              "      <td>1.551058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>1.616100</td>\n",
              "      <td>1.550956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>1.634500</td>\n",
              "      <td>1.551149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>1.361900</td>\n",
              "      <td>1.551121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.347300</td>\n",
              "      <td>1.551143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>1.560600</td>\n",
              "      <td>1.550923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>1.552700</td>\n",
              "      <td>1.551391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>1.835000</td>\n",
              "      <td>1.551262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>1.561900</td>\n",
              "      <td>1.551477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.456400</td>\n",
              "      <td>1.551470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>1.662100</td>\n",
              "      <td>1.551324</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1415, training_loss=1.7411934195474685, metrics={'train_runtime': 502.666, 'train_samples_per_second': 22.45, 'train_steps_per_second': 2.815, 'total_flos': 369863056097280.0, 'train_loss': 1.7411934195474685, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model and tokenizer"
      ],
      "metadata": {
        "id": "ZvKqNVGalhB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"FineTuned-lora-GPT2\")\n",
        "tokenizer.save_pretrained(\"FineTuned-lora-GPT2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1PDNUvua3J7",
        "outputId": "cd7e2800-0882-46f0-f704-e06516429112"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('FineTuned-lora-GPT2/tokenizer_config.json',\n",
              " 'FineTuned-lora-GPT2/special_tokens_map.json',\n",
              " 'FineTuned-lora-GPT2/vocab.json',\n",
              " 'FineTuned-lora-GPT2/merges.txt',\n",
              " 'FineTuned-lora-GPT2/added_tokens.json',\n",
              " 'FineTuned-lora-GPT2/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference From Saved Model"
      ],
      "metadata": {
        "id": "Bsu8W5M6lmMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "#  Load tokenizer\n",
        "base_model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"FineTuned-lora-GPT2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#  Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"FineTuned-lora-GPT2\")\n",
        "\n",
        "#  Build pipeline\n",
        "text_gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7RkMGS3a3G9",
        "outputId": "14564e63-aa2a-4fbe-f403-f98498dd5443"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The secret to happiness is\"\n",
        "outputs = text_gen(prompt, max_new_tokens=70, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuGoIHefa3D0",
        "outputId": "8cbcf520-2350-47d2-a3a5-0b3fb35c449a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The secret to happiness is to live life in harmony. There is no way to live without war. The only way to live is to live together. Only peace is possible when you live together.\n",
            "\n",
            "I wrote to him at one point about my love for his wife. In my mind's eye we read of two things that I love: love and war. I love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"once upon a time\"\n",
        "outputs = text_gen(prompt, max_new_tokens=70, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObkPlzWXkLnv",
        "outputId": "590cfe23-4a9d-4311-c6e9-4ec717ebaf5d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "once upon a time when you were just as good a person as you were a day later. Now, you're as bad as you were when you were a day earlier. And you're as bad as you were when you were a day earlier. And now, you're as bad as you were when you were a day earlier. And now, you're as bad\n"
          ]
        }
      ]
    }
  ]
}